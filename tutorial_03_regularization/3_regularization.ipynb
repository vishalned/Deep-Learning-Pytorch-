{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d9b11112e967a0b426f92979cc97288",
     "grade": false,
     "grade_id": "cell-51463f10ee76cd64",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "In this exercise, we will try a few methods to prevent overfitting of an MLP network to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression problem\n",
    "We will look at a regression problem where the task is to estimate a function of one variable\n",
    "$$y = f(x)$$\n",
    "using a set of training examples $(x_1, y_1), \\ldots, (x_n, y_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baeb8a3430abfe96373c944354ee3f75",
     "grade": false,
     "grade_id": "cell-602ebca68e883d0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us first generate training examples $y_i=\\sin(x_i) + n_i$ with $x_i$ drawn from the uniform distribution in $[-0.5, 0.5]$ and noise $n_i$ drawn from the Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "n = 150\n",
    "x = np.random.rand(n, 1)-0.5\n",
    "\n",
    "def fun(x):\n",
    "    y = np.cos(2* np.pi * x)\n",
    "    y += 0.3 * np.random.randn(*x.shape)\n",
    "    return y\n",
    "\n",
    "y = fun(x)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.tensor(x).float()\n",
    "y = torch.tensor(y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "083ad76a86a911e9119a37c5e664a35d",
     "grade": false,
     "grade_id": "cell-4ffc8ddcb1221702",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let us split the data into training, validation and test sets and plot the training and validation sets. And let us plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation and test sets\n",
    "torch.manual_seed(3)\n",
    "rp = torch.randperm(x.size(0))\n",
    "\n",
    "n_train = int(x.size(0) * 0.5)\n",
    "x_train, y_train = x[rp[:n_train]], y[rp[:n_train]]\n",
    "x_test, y_test = x[rp[n_train:]], y[rp[n_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fix, ax = plt.subplots(1)\n",
    "ax.plot(x_train, y_train, 'b.')\n",
    "ax.plot(x_test, y_test, 'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a85c70f5144240f62108369d9e9111c",
     "grade": false,
     "grade_id": "cell-14cf094e013cff8b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Define a multi-layer perceptron (MLP) network with two hidden layers\n",
    "\n",
    "In the code below, we define a neural network architecture with:\n",
    "* input dimension 1\n",
    "* one hidden layer with 100 units with tanh nonlinearity\n",
    "* one hidden layer with 100 units with tanh nonlinearity\n",
    "* linear output layer with output dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions to assess the quality of the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This visualizes the function implemented by an MLP\n",
    "def plot_fit(mlp, x, y):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(x, y, '.')\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(np.linspace(-0.5, 0.5, 100).reshape((-1, 1))).float()\n",
    "        pred = mlp.forward(x)\n",
    "    ax.plot(x, pred)\n",
    "\n",
    "# This is the function to compute the loss:\n",
    "def compute_loss(mlp, x, y):\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = mlp.forward(x)\n",
    "        loss = F.mse_loss(outputs, y)\n",
    "        return loss.cpu().numpy()\n",
    "\n",
    "# This is the function to print the progress during training\n",
    "def print_progress(epoch, train_error, test_error):\n",
    "    print(f'Epoch {epoch+1}: Train error: {train_error:.2f}, Test error: {test_error:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MLP\n",
    "mlp = MLP()\n",
    "\n",
    "# Plot the function implemented by the MLP\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0556343b1cc1fb1219188a2ccfa636da",
     "grade": false,
     "grade_id": "cell-38e864109561c7c3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Train the MLP network without regularization\n",
    "Training is done by minimizing the mean-squared error computed on the training data:\n",
    "$$c=\\sum_{i=1}^n || f(x_i) - y_i ||^2.$$\n",
    "\n",
    "Here, we train the network:\n",
    "* using all the data for computing the gradient (batch mode)\n",
    "* using `n_epochs` epochs (which is equal to the number of parameter updates in the batch mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n",
    "n_epochs = 10000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)\n",
    "plt.plot(x_test, y_test, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a108841c5fbfbb3d4eced5606a0ff05",
     "grade": false,
     "grade_id": "cell-3dbf989a13b25a8a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, the network overfits to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_no_regularization = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss without regularization: %.5f\" % test_loss_no_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "851eb7e44885c359e10589f00005f4cf",
     "grade": false,
     "grade_id": "cell-b6cf5359944d980f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Early stopping\n",
    "\n",
    "One of the simplest ways to avoid overfitting is to stop training when the validation error starts to grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us look at the learning curves: the evolution of training and validation errors during training\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3d8061a18ec107f1fcc8c4d7ec0c058",
     "grade": false,
     "grade_id": "cell-3d5bbf165b21b8aa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In the code below, define a stopping creterion in function `stop_criterion`. Training should be stopped (function returns  `True`) when the validation error is larger than the best validation error obtained so far (with given `tolerance`) for `patience` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          patience (int):    Maximum number of epochs with unsuccessful updates.\n",
    "          tolerance (float): We assume that the update is unsuccessful if the validation error is larger\n",
    "                              than the best validation error so far plus this tolerance.\n",
    "        \"\"\"\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    \n",
    "    def stop_criterion(self, test_errors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          test_errors (iterable): Validation errors after every update during training.\n",
    "        \n",
    "        Returns: True if training should be stopped: when the validation error is larger than the best\n",
    "                  validation error obtained so far (with given tolearance) for patience epochs (number of consecutive epochs for which the criterion is satisfied).\n",
    "                 \n",
    "                 Otherwise, False.\n",
    "        \"\"\"\n",
    "        if len(test_errors) <= self.patience:\n",
    "            return False\n",
    "\n",
    "        min_val_error = min(test_errors)\n",
    "        test_errors = np.array(test_errors[-self.patience:])\n",
    "        return all(test_errors > min_val_error + self.tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the network with early stopping\n",
    "mlp = MLP()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "n_epochs = 10000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "early_stop = EarlyStopping(tolerance=0.005, patience=20)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "    test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "\n",
    "    if early_stop.stop_criterion(test_errors):\n",
    "        print(test_errors[epoch])\n",
    "        print('Stop after %d epochs' % epoch)\n",
    "        break\n",
    "        \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print_progress(epoch, train_errors[epoch], test_errors[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves: the evolution of training and validation errors during training\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_early_stopping = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with early stopping: %.5f\" % test_loss_early_stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d70dd3d3f8e8c2cfb4b55f8c255c03e2",
     "grade": false,
     "grade_id": "cell-898f073dfa615c29",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Weight-decay regularization\n",
    "\n",
    "Let us train the same network with L2 penalties on the weights. In PyTorch, one can add L2 penalty terms for all the parameters by providing `weight_decay` argument for most types of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an MLP with L2 regularization\n",
    "mlp = MLP()\n",
    "\n",
    "# Create an Adam optimizer with learning rate 0.01 and weight decay parameter 0.001\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01, weight_decay=0.001)\n",
    "n_epochs = 4000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves (the evolution of the following quantities during training)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_weight_decay = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with weight decay: %.5f\" % test_loss_weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34a6e0abf991b63f9628c3317af8959a",
     "grade": false,
     "grade_id": "cell-838adccf05b5b869",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Injecting noise to inputs\n",
    "\n",
    "One way to improve generalization is to add noise to the inputs. So, we update the parameters of $f$ using the gradient of the following function\n",
    "$$c= \\sum_{i=1}^n || f(x_i + n_i) - y_i ||^2$$\n",
    "where $n_i$ is a noise instance. In the code, we corrupt input with Gaussian noise with standard deviation 0.07."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP with injecting noise to inputs\n",
    "mlp = MLP()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "n_epochs = 4000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train + 0.07 * torch.randn_like(x_train))\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves: the evolution of training and validation errors during training\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c6bb460c104299c6ebec89b7beecfd6",
     "grade": false,
     "grade_id": "cell-c8eb739eb3ecb6ec",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_inj_noise = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with noise injection: %.5f\" % test_loss_inj_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3f73fc6ac42f0352b08e6c23f44c88d",
     "grade": false,
     "grade_id": "cell-b194dea5c4db3b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "Another way to improve generalization is to use dropout. In the cell below, we define an MLP with exactly the same architecture as previously but with `nn.Dropout` layers (with dropout probability 0.2) after each `tanh`\n",
    "nonlinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPDropout, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(100, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPDropout()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)\n",
    "#scheduler = StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "n_epochs = 4000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #scheduler.step()\n",
    "    mlp.train()  # Dropout layers work differently during training and test. This sets the training mode.\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        mlp.eval()\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves (the evolution of the following quantities during training)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "mlp.eval()\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_dropout = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss with dropout: %.5f\" % test_loss_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd87d02f28cf3e69ce8cc91e556e575d",
     "grade": false,
     "grade_id": "cell-55bc3b6b8e363d8e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Reducing model capacity\n",
    "\n",
    "Another simple way to reduce overfitting is to reduce the capacity of the model. Let us use for the same regression task a much smaller network: an MLP with one hidden layer with five units, tanh nonlinearity in the hidden layer and a linear output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPSmall, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(5, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPSmall()\n",
    "\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "n_epochs = 10000\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mlp.forward(x_train)\n",
    "    loss = F.mse_loss(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        train_errors.append(compute_loss(mlp, x_train, y_train))\n",
    "        test_errors.append(compute_loss(mlp, x_test, y_test))\n",
    "        print_progress(epoch, train_errors[-1], test_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves (the evolution of the following quantities during training)\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.loglog(train_errors)\n",
    "ax.loglog(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final fit\n",
    "mlp.eval()\n",
    "plot_fit(mlp, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the test loss\n",
    "test_loss_small = compute_loss(mlp, x_test, y_test)\n",
    "print(\"Test loss by reducing model capacity: %.5f\" % test_loss_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can summarize the results obtained with different regularization methods:\n",
    "print('No regularization: %.5f' % test_loss_no_regularization)\n",
    "print('Early stopping:    %.5f' % test_loss_early_stopping)\n",
    "print('Weight decay:      %.5f' % test_loss_weight_decay)\n",
    "print('Noise injection:   %.5f' % test_loss_inj_noise)\n",
    "print('Dropout:           %.5f' % test_loss_dropout)\n",
    "print('Small network:     %.5f' % test_loss_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01e31500280375eed0bced9cefd41b0b",
     "grade": false,
     "grade_id": "cell-a9fc6f9740d33f87",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The values of the hyperparameters (parameters of the training procedure) may have a major impact on the results. The best hyperparameters are usually found by measuring the performance on the validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
